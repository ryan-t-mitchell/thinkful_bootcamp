{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data\n",
    "\n",
    "In this lesson we'll introduce you to the different concepts of missingness, and give a brief introduction to how to impute (replace) missing data.\n",
    "\n",
    "Some datasets may be perfectly complete, but many will arrive with some missing values. Cleaning can increase the amount of missing data. Even if missingness is random, it can cause difficulties for analysis. The Python implementations of basic statistical methods like ANOVA, t-tests, and correlations will fail if any of the variables involved has a missing value. One way to solve this problem is to drop any rows that contain missing values in your variables of interest. The pandas package has the .dropna() data frame method for doing exactly this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age gender  height  weight\n",
      "0  27.0      f    64.0   140.0\n",
      "1  50.0      f     NaN     NaN\n",
      "2  34.0      f    71.0   130.0\n",
      "3   NaN      m    66.0   110.0\n",
      "4   NaN      m    68.0   160.0\n",
      "5   NaN   None     NaN     NaN\n",
      "    age gender  height  weight\n",
      "0  27.0      f    64.0   140.0\n",
      "2  34.0      f    71.0   130.0\n",
      "    age gender  height  weight\n",
      "0  27.0      f    64.0   140.0\n",
      "1  50.0      f     NaN     NaN\n",
      "2  34.0      f    71.0   130.0\n",
      "3   NaN      m    66.0   110.0\n",
      "4   NaN      m    68.0   160.0\n",
      "    age gender  height  weight\n",
      "0  27.0      f    64.0   140.0\n",
      "1  50.0      f     NaN     NaN\n",
      "2  34.0      f    71.0   130.0\n",
      "3   NaN      m    66.0   110.0\n",
      "4   NaN      m    68.0   160.0\n",
      "    age gender  height  weight\n",
      "0  27.0      f    64.0   140.0\n",
      "2  34.0      f    71.0   130.0\n",
      "3   NaN      m    66.0   110.0\n",
      "4   NaN      m    68.0   160.0\n",
      "\n",
      "\n",
      "    age gender  height  weight\n",
      "0  27.0      f    64.0   140.0\n",
      "2  34.0      f    71.0   130.0\n",
      "3   NaN      m    66.0   110.0\n",
      "4   NaN      m    68.0   160.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data to play with and clean.\n",
    "data = {\n",
    "    'age': [27, 50, 34, None, None, None],\n",
    "    'gender': ['f', 'f', 'f', 'm', 'm', None],\n",
    "    'height' : [64, None, 71, 66, 68, None],\n",
    "    'weight' : [140, None, 130, 110, 160, None],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Full dataset.\n",
    "print(df)\n",
    "\n",
    "# Drop all rows that have any missing values in any column.\n",
    "print(df.dropna()) \n",
    "\n",
    "# Drop only rows where all values are missing.\n",
    "print(df.dropna(how='all'))\n",
    "\n",
    "# Drop only rows where more than two values are missing.\n",
    "print(df.dropna(thresh=2))\n",
    "\n",
    "# Drop all rows that have any missing values in the 'gender' or 'height' columns.\n",
    "print(df.dropna(subset=['gender','height']))\n",
    "\n",
    "# Your turn. Write code below to drop rows where both height and weight\n",
    "# are missing and print the result.\n",
    "print('\\n')\n",
    "print(df.dropna(subset = ['height', 'weight']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When Does Missingness Matter?\n",
    "\n",
    "Sometimes dropping all rows with missing data is fine, but sometimes it creates problems. Missing data matter if we believe the missingness will cause:\n",
    "\n",
    "Loss of statistical power because so many rows have to be thrown out, making it harder to detect effects, or\n",
    "Bias because certain values are more likely to be missing than others.\n",
    "\n",
    "To know when to worry about missing data and when to throw out incomplete cases and proceed as planned, see where the missingness falls in the following categories:\n",
    "\n",
    "Missing Completely at Random (\"MCAR\"):\n",
    "\n",
    "A catastrophic flood washed away some of the servers and 20% of the data was lost.\n",
    "\n",
    "Unless so much data is lost that sample sizes are now too small, it is fair to throw out the missing values and proceed.\n",
    "\n",
    "Missing at Random (\"MAR\"):\n",
    "\n",
    "Women are more likely to skip a question about weight, regardless of their actual weight.\n",
    "Because we can explain why the data is missing using data we have, we can proceed as long as we include the variable that \"explains\" the missingness in our analyses.\n",
    "\n",
    "There is no way to know that data is MAR, but sometimes we can assume it is. If we find a variable in our dataset that seems to differentiate really well between missing and non-missing (90% of the people with missing values on the \"depression\" score are men) we have reason to suspect MNAR.\n",
    "\n",
    "Missing Not at Random (\"MNAR\")\n",
    "\n",
    "LGBT individuals less likely to answer a survey question about their sexual orientation.\n",
    "\n",
    "Systematic missingness: People who would answer in a certain way (LGBT vs. Heterosexual) are less likely to answer at all.\n",
    "\n",
    "Stop, do not pass Go, do not collect $200. If we throw out MNAR data, we end up with a biased sample (proportionately fewer LGBT people than in the population we want to study) and biased conclusions.\n",
    "\n",
    "Note that since, by definition, we don't know what people would have said for questions they don't answer, MNAR is an assumption based on looking at the data and noticing what isn't there: Abnormally low counts of LGBT people, almost no men who say they are depressed, variables with missingness where nobody picks the highest or lowest value, etc.\n",
    "What do you do if you have MNAR data you can't drop (or if it is MCAR or MAR but dropping missing values leaves your sample too small)?\n",
    "\n",
    "# Imputing Data\n",
    "\n",
    "In cases where we want to keep all the information from all rows, even incomplete ones, we can \"guess\" what the missing data would have been and fill in that cell with our guess. This approach is called imputation.\n",
    "\n",
    "There are many methods for imputing data, from the simple to the very complex. The most straightforward involves replacing missing values with the mode, mean, or median of the variable. This method isn't perfect: it keeps central tendency the same, but reduces variance and correlations among variables. Here's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    age gender  height  weight\n",
      "0  27.0      f   64.00   140.0\n",
      "1  50.0      f   67.25   135.0\n",
      "2  34.0      f   71.00   130.0\n",
      "3  37.0      m   66.00   110.0\n",
      "4  37.0      m   68.00   160.0\n",
      "5  37.0   None   67.25   135.0\n",
      "    age gender  height  weight\n",
      "0  27.0      f    64.0   140.0\n",
      "1  50.0      f    68.0   160.0\n",
      "2  34.0      f    71.0   130.0\n",
      "3  34.0      m    66.0   110.0\n",
      "4  34.0      m    68.0   160.0\n",
      "5  34.0      f    68.0   160.0\n",
      "    age gender  height  weight\n",
      "0  27.0      f    64.0   140.0\n",
      "1  50.0      f    68.0   160.0\n",
      "2  34.0      f    71.0   130.0\n",
      "3  34.0      m    66.0   110.0\n",
      "4  34.0      m    68.0   160.0\n",
      "5  34.0      f    68.0   160.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample data to play with.\n",
    "data = {\n",
    "    'age': [27, 50, 34, None, None, None],\n",
    "    'gender': ['f', 'f', 'f', 'm', 'm', None],\n",
    "    'height' : [64, None, 71, 66, 68, None],\n",
    "    'weight' : [140, None, 130, 110, 160, None],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# For each numeric column, replace the missing values with the mean for that column.\n",
    "df.fillna(df.mean(),inplace=True)\n",
    "print(df)\n",
    "\n",
    "# For each column, replace the missing values with the most common value for that\n",
    "# column. Useful for filling in missing categorical values.\n",
    "# As written, this command will fill in missing values for both numerical and\n",
    "# categorical columns.\n",
    "df = pd.DataFrame(data)\n",
    "df = df.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "print(df)\n",
    "\n",
    "# Your turn. Try replacing each value with the median, mode, or other statistic\n",
    "# of your choice.\n",
    "\n",
    "df.fillna(df.median(), inplace = True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to see a more sophisticated method of replacing missing data, involving grouping existing entries into \"similar\" groups and filling in the missing values within a group with the mean for that group, check out this in-depth tutorial.\n",
    "\n",
    "Imputation is a deep and complex topic, and will be discussed more in Unit 5 as one of the optional specializations.\n",
    "\n",
    "# Beyond Imputation\n",
    "\n",
    "If the causes of MNAR (or of major, catastrophic amounts of missingness that is MCAR or MAR) are clear and easy to fix, then fixing those causes and collecting new data may be easier than imputation. Either run the study afresh, or collect more data with an intentional focus on the groups with highest missingness. For example, if a coding error in a tech usage survey means data wasn't recorded for any Mac users, it may be easier to fix the coding error and run the study again (or fix the coding error and collect data from just Mac users) than try to impute such a centrally important variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap up\n",
    "\n",
    "After completing this lesson, you should feel comfortable creating attractive, clean plots and subplots using Seaborn, and know which plots to choose to highlight various data features. You should also understand how visualization is critical to data cleaning. Finally, you should be able to explain how various kinds of dirty and missing data threaten the validity of analyses, and implement some basic approaches for dealing with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
