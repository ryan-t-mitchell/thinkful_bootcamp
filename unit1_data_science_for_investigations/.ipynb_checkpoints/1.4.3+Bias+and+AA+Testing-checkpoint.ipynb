{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias and A/A Testing\n",
    "\n",
    "In experiments, bias refers to anything that causes a sample to systematically differ from the population. Bias can come from a) the sampling procedure, b) the assignment to conditions procedure, c) the context of the study, or d) the people running the study.\n",
    "\n",
    "You may remember sampling bias, also called selection bias, from our discussion in the fundamentals course. When a sample differs from the population in some way – more women, younger, richer, more outgoing, etc – then conclusions based on that sample may not apply to the population. Selection bias can be avoided by choosing the sample randomly from the population.\n",
    "\n",
    "Bias in assignment to conditions is similar to selection bias because it also represents a failure of randomness. In this case, the sample is supposed to be separated into groups that are similar to one another. The groups then each experience one of the study conditions. If the groups are the same before they go into the study, then any differences between the groups during the study must be due to the differences you introduce as part of the experiment and you can conclude that people in the population will react similarly to your experimental stimuli.\n",
    "\n",
    "For example, imagine you are comparing the effect of sprinkles on liking ice cream. One group is given ice cream with sprinkles, and one is given plain ice cream. Both groups rate how much they like the ice cream, and it turns out the ice cream with sprinkles was liked more. If the groups are similar, then you can conclude that sprinkles cause people to like ice cream more. If the groups are different – for example, if the sprinkles group had an average age of 13, and the plain group had an average age of 30 – then it isn't clear whether sprinkles cause liking, or whether younger people just like ice cream more. Conclusions about the effect of sprinkles on ice cream are biased by the difference in ages of the groups.\n",
    "\n",
    "Bias can also come from the study context, or setting. Imagine you have a great sample that is very similar to the population, and you divide them into two groups, who are very similar to one another. Then you separate the groups into two rooms, and ask them to read and rate some ads for sodas. You are interested in how small differences between the ads seen by groups 1 and 2 will affect their reactions when they leave the rooms and are offered a soda afterward. To your surprise, group 1 overwhelmingly thanks you for the soda and drinks up immediately, while only half of group 2 accept the soda. You conclude that the ads for group 1 must be better. Unfortunately, what you don't know is that the heater was broken in group 1's room, and the temperature was 90 degrees. This contextual factor biased your results. Not all sources of contextual bias are this obvious, but it underscores the importance of matching the setting for all experimental groups.\n",
    "\n",
    "Finally, bias can come from the people running the study. This is a broad category, running the gamut from subtle signs (during an interview, the interviewer smiles when the participants give answers she likes, and frowns otherwise) to blatant interference (not recording answers that clash with a desired conclusion). Observer bias is tough to eliminate. The best way to avoid it is to make sure that the people who interact with study participants don't expect a particular result.\n",
    "\n",
    "# A/A Testing\n",
    "Of these sources of bias, all but selection bias may be detectable using A/A testing.\n",
    "\n",
    "A/A testing provides context for the results of A/B testing. As the name suggests, A/A testing means comparing the outcome of choice between two identical versions of something. This might seem rather silly – after all, if we divide all newsletter subscribers into two groups but send both groups the same email, we would expect that clickthrough rates for both groups would be identical. In practice, however, that doesn't always happen.\n",
    "\n",
    "Why is this useful? A/A testing can identify problems with:\n",
    "\n",
    "The testing method. Perhaps all Version 1 emails were accidentally set to go out in the morning when people are at work, while all Version 2 emails go out in the evening when people have leisure to shop.\n",
    "\n",
    "The random split method. Maybe the mailing list is arranged chronologically by date subscribed and Version 2 was sent to first half of list with the long-term customers who are more likely to click.\n",
    "The size of the sample. The rarer the outcome, the bigger sample needed to get an accurate estimate of the response rate.\n",
    "\n",
    "Any other outside factors that can affect the outcome of a test.\n",
    "In addition, if there are no bugs or problems, the size of the difference between groups in an A/A test can be used to define a threshold for subsequent A/B testing, where differences between groups smaller than the difference observed in the A/A test are assumed to be due to chance.\n",
    "\n",
    "# DRILL: Am I Biased?\n",
    "For each of the following scenarios, call out the potential biases in the proposed experiment. Do your best to try to discover not only the bias, but the initial design. There is plenty of room for interpretation here, so make sure to state what assumptions you're making.\n",
    "\n",
    "# You're testing advertising emails for a bathing suit company and you test one version of the email in February and the other in May.\n",
    "\n",
    "Obviously the seasonality will imply a much larger response in May than February due to warmer temperatures and more beach vacations. Also, 3 months is a long time to separate test emails by - you could have considerable customer turnover & new customer acquisitions that would make the CM mix vastly different between the emails. Time of day of email is not mentioned, but is important to know. \n",
    "\n",
    "\n",
    "# You open a clinic to treat anxiety and find that the people who visit show a higher rate of anxiety than the general population.\n",
    "\n",
    "The design of this experiment is flawed from the outset; clearly you would expect a greater percentage of people with anxiety visiting a clinic specializing in this disorder compared to the general population. \n",
    "\n",
    "# You launch a new ad billboard based campaign and see an increase in website visits in the first week.\n",
    "\n",
    "Are the results adjusted for seasonality? Assuming so, is there any way to measure the channel through which this increased traffic has entered the site (some way to link it to the bilboard campaign?)? \n",
    "\n",
    "# You launch a loyalty program but see no change in visits in the first week.\n",
    "\n",
    "If you launched the campaign on your site (and had no other way of informing customers about the change outside of your site), then you should not expect any change in visits in the near term because the customers are presumably not even aware of the campaign until they hit the site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
