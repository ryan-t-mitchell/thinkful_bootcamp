{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: Iterate and evaluate your classifier\n",
    "It's time to revisit your classifier from the previous assignment. Using the evaluation techniques we've covered here, look at your classifier's performance in more detail. Then go back and iterate by engineering new features, removing poor features, or tuning parameters. Repeat this process until you have five different versions of your classifier. Once you've iterated, answer these questions to compare the performance of each:\n",
    "\n",
    "Do any of your classifiers seem to overfit?\n",
    "\n",
    "Which seem to perform the best? Why?\n",
    "\n",
    "Which features seemed to be most impactful to performance?\n",
    "\n",
    "Write up your iterations and answers to the above questions in a few pages. Submit a link below and go over it with your mentor to see if they have any other ideas on how you could improve your classifier's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A very, very, very slow-moving, aimless movie ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not sure who was more lost - the flat characte...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attempting artiness with black &amp; white and cle...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Very little music or anything to speak of.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best scene in the movie was when Gerardo i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The rest of the movie lacks art, charm, meanin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wasted two hours.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Saw the movie today and thought it was a good ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A bit predictable.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Loved the casting of Jimmy Buffet as the scien...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  positive\n",
       "0  A very, very, very slow-moving, aimless movie ...         0\n",
       "1  Not sure who was more lost - the flat characte...         0\n",
       "2  Attempting artiness with black & white and cle...         0\n",
       "3       Very little music or anything to speak of.           0\n",
       "4  The best scene in the movie was when Gerardo i...         1\n",
       "5  The rest of the movie lacks art, charm, meanin...         0\n",
       "6                                Wasted two hours.           0\n",
       "7  Saw the movie today and thought it was a good ...         1\n",
       "8                               A bit predictable.           0\n",
       "9  Loved the casting of Jimmy Buffet as the scien...         1"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "imdb = pd.read_csv('~/thinkful_mac/thinkful_large_files/imdb_labelled.csv', header = None)\n",
    "imdb.columns = ['review', 'positive']\n",
    "imdb.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1\n",
    "\n",
    "Original list (already iterated based on examination of IMDB ratings file, trial & error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 748 predictions, 150 were misclassified\n",
      "Accuracy: 79.95%\n"
     ]
    }
   ],
   "source": [
    "keywords = ['terrible', 'awful', 'worst', 'bad', 'stupid', 'poor', 'worse', 'attempt', 'crap', 'fail', 'annoying', 'cheap',\n",
    "           'painful', 'avoid', 'slow', 'pretentious', 'problem', 'embarrassing', 'bored', 'horrible', 'lousy', 'unfortunate', \n",
    "           'boring', 'sucks', 'sucked', 'waste', 'unbear', ' mess ', 'wasting', 'mediocre', 'sloppy',\n",
    "           'disappoint', 'garbage', 'whine', 'whiny', 'plot', 'hate ', 'hated', 'negative', 'nobody', 'flaw',\n",
    "           'script', 'insult', 'do not', 'torture', ' lack', 'lame', 'ridiculous', 'not', 'unbelievable', 'skip', 'shame', \n",
    "           'not even', 'miss', 'excellent', 'amazing', 'love', 'incredible', 'fantastic', 'terrific', 'best', 'great', 'fun',\n",
    "           'beautiful', 'well done', 'enjoy', 'perfect', 'smart', 'highly', 'impress', 'well']\n",
    "\n",
    "# Removed the required space before/after the keyword to improve model accuracy (many sentences in IMDB dataset began with\n",
    "#these words, so no space in front)\n",
    "for key in keywords:\n",
    "    imdb[str(key)] = imdb.review.str.contains(str(key), case = False)\n",
    "\n",
    "imdb['positive'] = (imdb['positive'] == 1)\n",
    "    \n",
    "data = imdb[keywords]\n",
    "target = imdb['positive']\n",
    "\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "pred = bnb.predict(data)\n",
    "\n",
    "print('Out of {} predictions, {} were misclassified'.format(data.shape[0], (pred != target).sum()))\n",
    "print('Accuracy: {}'.format(format(100*(target == pred).sum()/len(pred), '0.2f'))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[240 122]\n",
      " [ 28 358]]\n",
      "The accuracy of the model is:  0.7994652406417112\n",
      "The sensitivity (Percentage of positives correctly predicted) of the model is: 0.927461139896373\n",
      "The specificity (Percentage of negatives correctly predicted) of the model is: 0.6629834254143646\n"
     ]
    }
   ],
   "source": [
    "#Test the accuracy, sensitivity, and specificity of MODEL 1 (original model)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(target, pred)\n",
    "\n",
    "print('Confusion Matrix: \\n{}'.format(c))\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model is: ', 1-((pred != target).sum()/data.shape[0]))\n",
    "\n",
    "#Sensitivity\n",
    "print('The sensitivity (Percentage of positives correctly predicted) of the model is: {}'.format((c[1][1])/(c[1][1] + c[1][0])))\n",
    "\n",
    "#Specificity\n",
    "print('The specificity (Percentage of negatives correctly predicted) of the model is: {}'.format((c[0][0])/(c[0][0] + c[0][1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run some cross validation on Model 1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.00199032, 0.00159693, 0.00150824]),\n",
       " 'score_time': array([0.00050378, 0.00042605, 0.00039506]),\n",
       " 'test_score': array([0.736     , 0.744     , 0.75403226]),\n",
       " 'train_score': array([0.80120482, 0.79317269, 0.798     ])}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "bnb = BernoulliNB()\n",
    "cross_validate(bnb, data, target, cv = 3, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Given the results of the cross-validation, there is a small amount of over-fitting for my initial model. Accuracies ranged from 73.6% to 75.4% on the test sets using 3 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2\n",
    "\n",
    "Try to minimize false positives (minimize the number of reviews tagged as negative that are actually positive). In this instance, we don't care as much about accuracy as we do about categorizing a negative review incorrectly...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[ 47 315]\n",
      " [  0 386]]\n",
      "The accuracy of the model is:  0.5788770053475936\n",
      "The sensitivity (Percentage of positives correctly predicted) of the model is: 1.0\n",
      "The specificity (Percentage of negatives correctly predicted) of the model is: 0.1298342541436464\n"
     ]
    }
   ],
   "source": [
    "# So let's try predicting by only using a very negative list of words\n",
    "\n",
    "keywords = ['awful', 'worst', 'trash', 'painful', 'sloppy', 'pretentious', 'embarrassing', 'hate', 'torture', 'skip']\n",
    "\n",
    "for key in keywords:\n",
    "    imdb[str(key)] = imdb.review.str.contains(str(key), case = False)\n",
    "\n",
    "imdb['positive'] = (imdb['positive'] == 1)\n",
    "    \n",
    "data = imdb[keywords]\n",
    "target = imdb['positive']\n",
    "\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "pred = bnb.predict(data)\n",
    "            \n",
    "#Test the accuracy, sensitivity, and specificity\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(target, pred)\n",
    "\n",
    "print('Confusion Matrix: \\n{}'.format(c))\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model is: ', 1-((pred != target).sum()/data.shape[0]))\n",
    "\n",
    "#Sensitivity\n",
    "print('The sensitivity (Percentage of positives correctly predicted) of the model is: {}'.format((c[1][1])/(c[1][1] + c[1][0])))\n",
    "\n",
    "#Specificity\n",
    "print('The specificity (Percentage of negatives correctly predicted) of the model is: {}'.format((c[0][0])/(c[0][0] + c[0][1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model was actually pretty terrible - it did not do what I thought it would, which is predict negatives correctly! In hindsight, the reason for this is clear: the keyword list simply does not capture enough of the negative descriptors. Basically, if the review contains one of these words, it will be flagged as negative, but so many of the negative reviews don't contain these words, and hence will be marked as positive (the dominant class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3\n",
    "Try to maximize accuracy using positive sentiment wordlist from internet (words from http://ptrckprry.com/course/ssd/data/positive-words.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/thinkful_mac/thinkful_large_files/positive_word_list_from_internet.csv', header=None)\n",
    "df.columns = ['positive_sentiment_list']\n",
    "pos_list = df['positive_sentiment_list'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[340  22]\n",
      " [142 244]]\n",
      "The accuracy of the model is:  0.7807486631016043\n",
      "The sensitivity (Percentage of positives correctly predicted) of the model is: 0.6321243523316062\n",
      "The specificity (Percentage of negatives correctly predicted) of the model is: 0.9392265193370166\n"
     ]
    }
   ],
   "source": [
    "imdb = pd.read_csv('~/thinkful_mac/thinkful_large_files/imdb_labelled.csv', header = None)\n",
    "# Renamed column from 'positive' to 'positive_review' due to the word being present in the keyword list\n",
    "imdb.columns = ['review', 'positive_review']\n",
    "\n",
    "keywords = pos_list\n",
    "\n",
    "for key in keywords:\n",
    "    imdb[str(key)] = imdb.review.str.contains(str(key), case = False)\n",
    "\n",
    "imdb['positive_review'] = (imdb['positive_review'] == 1)\n",
    "    \n",
    "data = imdb[keywords]\n",
    "target = imdb['positive_review']\n",
    "\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "pred = bnb.predict(data)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(target, pred)\n",
    "\n",
    "print('Confusion Matrix: \\n{}'.format(c))\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model is: ', 1-((pred != target).sum()/data.shape[0]))\n",
    "\n",
    "#Sensitivity\n",
    "print('The sensitivity (Percentage of positives correctly predicted) of the model is: {}'.format((c[1][1])/(c[1][1] + c[1][0])))\n",
    "\n",
    "#Specificity\n",
    "print('The specificity (Percentage of negatives correctly predicted) of the model is: {}'.format((c[0][0])/(c[0][0] + c[0][1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Positive keyword list was slightly less accurate than my model (Model 1). Sensitivity and specificity were lower and higher, respectively (the scores were basically reversed from mine -- very interesting). What about it's cross-validation performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.01265931, 0.01078606, 0.00988102]),\n",
       " 'score_time': array([0.00759578, 0.00451303, 0.00364494]),\n",
       " 'test_score': array([0.668     , 0.72      , 0.65322581]),\n",
       " 'train_score': array([0.80923695, 0.79919679, 0.846     ])}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's see how well the model accuracy stands up to cross-validation. \n",
    "\n",
    "cross_validate(bnb, data, target, cv=3, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Model 3, with only positive keywords, suffered when performing cross-validation. The data is over-fitting: our test scores ranged from 65-72% accuracy (versus 78% on the whole dataset). This is worse than my initial model (Model 1), BUT that model is performing better in the cross-validations because the list of keywords was curated from looking through all samples, so not exactly fair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 4 \n",
    "Try to maximize accuracy by using negative sentiment wordlist from internet (words from http://ptrckprry.com/course/ssd/data/negative-words.txt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('~/thinkful_mac/thinkful_large_files/negative_word_list_from_internet.csv', header=None)\n",
    "df.columns = ['negative_sentiment_list']\n",
    "\n",
    "# The list contains several characters that we need to replace in order for the code to run, including \"-\" and \"*\"\n",
    "df['negative_sentiment_list'] = df['negative_sentiment_list'].apply(lambda x: str(x).replace(\"-\", \"\"))\n",
    "df['negative_sentiment_list'] = df['negative_sentiment_list'].apply(lambda x: str(x).replace(\"*\", \"\"))\n",
    "neg_list = df['negative_sentiment_list'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[174 188]\n",
      " [  5 381]]\n",
      "The accuracy of the model is:  0.7419786096256684\n",
      "The sensitivity (Percentage of positives correctly predicted) of the model is: 0.9870466321243523\n",
      "The specificity (Percentage of negatives correctly predicted) of the model is: 0.48066298342541436\n"
     ]
    }
   ],
   "source": [
    "imdb = pd.read_csv('~/thinkful_mac/thinkful_large_files/imdb_labelled.csv', header = None)\n",
    "imdb.columns = ['review', 'positive']\n",
    "\n",
    "keywords = neg_list\n",
    "\n",
    "for key in keywords:\n",
    "    imdb[str(key)] = imdb.review.str.contains(str(key), case = False)\n",
    "    \n",
    "imdb['positive'] = (imdb['positive'] == 1)\n",
    "\n",
    "data = imdb[keywords]\n",
    "target = imdb['positive']\n",
    "\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "pred = bnb.predict(data)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(target, pred)\n",
    "\n",
    "print('Confusion Matrix: \\n{}'.format(c))\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model is: ', 1-((pred != target).sum()/data.shape[0]))\n",
    "\n",
    "#Sensitivity\n",
    "print('The sensitivity (Percentage of positives correctly predicted) of the model is: {}'.format((c[1][1])/(c[1][1] + c[1][0])))\n",
    "\n",
    "#Specificity\n",
    "print('The specificity (Percentage of negatives correctly predicted) of the model is: {}'.format((c[0][0])/(c[0][0] + c[0][1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.03428984, 0.02639675, 0.0234828 ]),\n",
       " 'score_time': array([0.02043509, 0.01096606, 0.01031399]),\n",
       " 'test_score': array([0.612     , 0.56      , 0.54435484]),\n",
       " 'train_score': array([0.74297189, 0.64457831, 0.672     ])}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's see how well the model accuracy stands up to cross-validation.\n",
    "\n",
    "cross_validate(bnb, data, target, cv=3, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the negative keyword list also suffered on cross-validation, with test scores in the 54-61% accuracy range (compared to 74% for the entire dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 5\n",
    "Positive & Negative sentiment lists combined (from internet, not my list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "[[230 132]\n",
      " [  6 380]]\n",
      "The accuracy of the model is:  0.8155080213903744\n",
      "The sensitivity (Percentage of positives correctly predicted) of the model is: 0.9844559585492227\n",
      "The specificity (Percentage of negatives correctly predicted) of the model is: 0.6353591160220995\n"
     ]
    }
   ],
   "source": [
    "imdb = pd.read_csv('~/thinkful_mac/thinkful_large_files/imdb_labelled.csv', header = None)\n",
    "imdb.columns = ['review', 'positive_review']\n",
    "\n",
    "posneg_list = pos_list + neg_list\n",
    "\n",
    "keywords = posneg_list\n",
    "\n",
    "for key in keywords:\n",
    "    imdb[str(key)] = imdb.review.str.contains(str(key), case = False)\n",
    "\n",
    "imdb['positive_review'] = (imdb['positive_review'] == 1)\n",
    "    \n",
    "data = imdb[keywords]\n",
    "target = imdb['positive_review']\n",
    "\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "pred = bnb.predict(data)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "c = confusion_matrix(target, pred)\n",
    "\n",
    "print('Confusion Matrix: \\n{}'.format(c))\n",
    "\n",
    "#Accuracy\n",
    "print('The accuracy of the model is: ', 1-((pred != target).sum()/data.shape[0]))\n",
    "\n",
    "#Sensitivity\n",
    "print('The sensitivity (Percentage of positives correctly predicted) of the model is: {}'.format((c[1][1])/(c[1][1] + c[1][0])))\n",
    "\n",
    "#Specificity\n",
    "print('The specificity (Percentage of negatives correctly predicted) of the model is: {}'.format((c[0][0])/(c[0][0] + c[0][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The model performs best with both positive and negative keywords, and this model outperformed my original model in terms of accuracy. Let's see how it does on cross-validation.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fit_time': array([0.04736495, 0.03517675, 0.03415608]),\n",
       " 'score_time': array([0.02315116, 0.01321006, 0.01299405]),\n",
       " 'test_score': array([0.7       , 0.592     , 0.56854839]),\n",
       " 'train_score': array([0.86144578, 0.69879518, 0.714     ])}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's see how well the model accuracy stands up to cross-validation. \n",
    "\n",
    "cross_validate(bnb, data, target, cv=3, return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 5 performs better than model 4 in the cross-validations with test scores ranging from 56-70%, but not as well as Model 3 which only used positive keywords and had cross-validation scores in the 65-72% range."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
