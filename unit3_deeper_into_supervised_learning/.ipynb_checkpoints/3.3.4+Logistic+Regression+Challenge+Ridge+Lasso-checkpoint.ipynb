{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "Now that you have two new regression methods at your fingertips, it's time to give them a spin. In fact, for this challenge, let's put them together! Pick a dataset of your choice with a binary outcome and the potential for at least 15 features. If you're drawing a blank, the crime rates in 2013 dataset has a lot of variables that could be made into a modelable binary outcome.\n",
    "\n",
    "Engineer your features, then create three models. Each model will be run on a training set and a test-set (or multiple test-sets, if you take a folds approach). The models should be:\n",
    "\n",
    "1) Vanilla logistic regression\n",
    "\n",
    "2) Ridge logistic regression\n",
    "\n",
    "3) Lasso logistic regression\n",
    "\n",
    "If you're stuck on how to begin combining your two new modeling skills, here's a hint: the SKlearn LogisticRegression method has a \"penalty\" argument that takes either 'l1' or 'l2' as a value.\n",
    "\n",
    "In your report, evaluate all three models and decide on your best. Be clear about the decisions you made that led to these models (feature selection, regularization parameter selection, model evaluation criteria) and why you think that particular model is the best of the three. Also reflect on the strengths and limitations of regression as a modeling approach. Were there things you couldn't do but you wish you could have done?\n",
    "\n",
    "Record your work and reflections in a notebook to discuss with your mentor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; \n",
    "sns.set();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "      ...       texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0     ...               17.33           184.60      2019.0            0.1622   \n",
       "1     ...               23.41           158.80      1956.0            0.1238   \n",
       "2     ...               25.53           152.50      1709.0            0.1444   \n",
       "3     ...               26.50            98.87       567.7            0.2098   \n",
       "4     ...               16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://www.kaggle.com/uciml/breast-cancer-wisconsin-data\n",
    "df = pd.read_csv('~/thinkful_mac/thinkful_large_files/breast_cancer_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Attribute Information:\n",
    "#1) ID number\n",
    "#2) Diagnosis (M = malignant, B = benign)\n",
    "#3-32 Ten real-valued features are computed for each cell nucleus:\n",
    "#a) radius (mean of distances from center to points on the perimeter)\n",
    "#b) texture (standard deviation of gray-scale values)\n",
    "#c) perimeter\n",
    "#d) area\n",
    "#e) smoothness (local variation in radius lengths)\n",
    "#f) compactness (perimeter^2 / area - 1.0)\n",
    "#g). concavity (severity of concave portions of the contour)\n",
    "#h). concave points (number of concave portions of the contour)\n",
    "#i). symmetry\n",
    "#j). fractal dimension (\"coastline approximation\" - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Information\n",
    "This dataset has 10 'unique' features, and three versions of these features (mean, SE, and 'worst', meaning the average of the 3 largest)\n",
    "\n",
    "The point is, the data should show some multicolinearity..meaning that our ridge regression should come in handy here, and that we could see some higher than usual coefficients in the regular logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert diagnosis to binary from M (malignant) and B (benign)\n",
    "df['diagnosis'] = df['diagnosis'].apply(lambda x: 1 if x == 'M' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
       "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
       "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
       "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
       "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
       "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
       "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
       "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
       "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df['Unnamed: 32'].unique())\n",
    "#All values in this column are NAN, so drop it, and also drop the ID column\n",
    "\n",
    "df.drop('Unnamed: 32', axis = 1, inplace = True)\n",
    "df.drop('id', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0          1        17.99         10.38          122.80     1001.0   \n",
       "1          1        20.57         17.77          132.90     1326.0   \n",
       "2          1        19.69         21.25          130.00     1203.0   \n",
       "3          1        11.42         20.38           77.58      386.1   \n",
       "4          1        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean           ...             radius_worst  texture_worst  \\\n",
       "0         0.2419           ...                    25.38          17.33   \n",
       "1         0.1812           ...                    24.99          23.41   \n",
       "2         0.2069           ...                    23.57          25.53   \n",
       "3         0.2597           ...                    14.91          26.50   \n",
       "4         0.1809           ...                    22.54          16.67   \n",
       "\n",
       "   perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "0           184.60      2019.0            0.1622             0.6656   \n",
       "1           158.80      1956.0            0.1238             0.1866   \n",
       "2           152.50      1709.0            0.1444             0.4245   \n",
       "3            98.87       567.7            0.2098             0.8663   \n",
       "4           152.20      1575.0            0.1374             0.2050   \n",
       "\n",
       "   concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0           0.7119                0.2654          0.4601   \n",
       "1           0.2416                0.1860          0.2750   \n",
       "2           0.4504                0.2430          0.3613   \n",
       "3           0.6869                0.2575          0.6638   \n",
       "4           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  \n",
       "0                  0.11890  \n",
       "1                  0.08902  \n",
       "2                  0.08758  \n",
       "3                  0.17300  \n",
       "4                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Train Percentage Accuracy\n",
      "0.9753954305799648 \n",
      "\n",
      "Accuracy: Fold 1\n",
      "0.9210526315789473\n",
      "Accuracy: Fold 2\n",
      "0.9649122807017544\n",
      "Accuracy: Fold 3\n",
      "0.9736842105263158\n",
      "Accuracy: Fold 4\n",
      "0.9736842105263158\n",
      "Accuracy: Fold 5\n",
      "0.9557522123893806\n",
      "\n",
      "Average CV accuracy: \n",
      " 0.9578171091445429\n",
      "\n",
      "Parameter estimates: \n",
      "\n",
      "[-4.80658975 -0.03294645  0.13337399  0.02657818  0.91058688  0.89125386\n",
      "  1.73859178  1.68164055  1.01864149  0.01391087  0.14729246 -2.94092035\n",
      "  0.50738884  0.10992636  0.14041741 -0.79178585 -0.86491466  0.18585906\n",
      "  0.11011728 -0.15773857 -0.06919903  0.45854925  0.10663872  0.01677184\n",
      "  1.92657661  2.16735943  3.78477668  3.35400848  2.80998972  0.23977778\n",
      " -1.22668934]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#REGULAR LOGISTIC REGRESSION\n",
    "\n",
    "predictors = df.iloc[:, 1:]\n",
    "targets = df['diagnosis']\n",
    "\n",
    "#Set the C value very high such that there is essentially no regularization taking place (C is inverse of regularization strength)\n",
    "lr = LogisticRegression(C = 1e9)\n",
    "\n",
    "# Fit the model.\n",
    "fit = lr.fit(predictors, targets)\n",
    "\n",
    "print('Overall Train Percentage Accuracy')\n",
    "print(lr.score(predictors, targets),'\\n')\n",
    "\n",
    "X = predictors\n",
    "y = targets\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "test_no = 1\n",
    "kf = KFold(n_splits=5) \n",
    "\n",
    "acc = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    #Use model defined above\n",
    "    predicted = lr.fit(X_train, y_train).predict(X_test)\n",
    "    actual = y_test\n",
    "    print('Accuracy: Fold', test_no)\n",
    "    print(lr.score(X_test, y_test))\n",
    "    acc.append(lr.score(X_test, y_test))\n",
    "    test_no = test_no + 1\n",
    "\n",
    "print('\\nAverage CV accuracy: \\n',np.mean(acc))    \n",
    "\n",
    "#Store the parameter estimates.\n",
    "origparams = np.append(lr.coef_, lr.intercept_)\n",
    "print('\\nParameter estimates: \\n')\n",
    "print(origparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In your report, evaluate all three models and decide on your best. Be clear about the decisions you made that led \n",
    "to these models (feature selection, regularization parameter selection, model evaluation criteria) and why you \n",
    "think that particular model is the best of the three. Also reflect on the strengths and limitations of regression as a modeling approach. Were there things you couldn't do but you wish you could have done?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Percentage Accuracy\n",
      "0.961335676625659 \n",
      "\n",
      "Accuracy: Fold 1\n",
      "0.9122807017543859\n",
      "Accuracy: Fold 2\n",
      "0.9736842105263158\n",
      "Accuracy: Fold 3\n",
      "0.9736842105263158\n",
      "Accuracy: Fold 4\n",
      "0.9649122807017544\n",
      "Accuracy: Fold 5\n",
      "0.9469026548672567\n",
      "\n",
      "Average CV accuracy: \n",
      " 0.9542928116752056\n",
      "\n",
      "Parameter estimates: \n",
      "\n",
      "[-2.41673843  0.03787475 -0.01877681  0.0069718   0.19397394  0.49799715\n",
      "  0.72981043  0.40337224  0.24143464  0.03929995  0.01610378 -1.71540785\n",
      " -0.06201418  0.11453601  0.02151266 -0.00315313  0.04596985  0.04851242\n",
      "  0.05267721 -0.01000586 -1.48318618  0.30102184  0.22551172  0.02496025\n",
      "  0.37927817  1.57091275  2.07514229  0.82097632  0.81770313  0.17115496\n",
      " -0.43111529]\n"
     ]
    }
   ],
   "source": [
    "#Now try ridge regression to see if adding a regularization penalty can help improve accuracy\n",
    "\n",
    "#RIDGE REGRESSION (L2)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "#Set the C value lower such that the regularization does have an impact\n",
    "lr = LogisticRegressionCV(penalty='l2', solver = 'liblinear', cv = 5)\n",
    "\n",
    "# Fit the model.\n",
    "fit = lr.fit(predictors, targets)\n",
    "\n",
    "print('Overall Percentage Accuracy')\n",
    "print(lr.score(predictors, targets),'\\n')\n",
    "\n",
    "X = predictors\n",
    "y = targets\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "test_no = 1\n",
    "kf = KFold(n_splits=5) \n",
    "\n",
    "acc = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    #Use model defined above\n",
    "    predicted = lr.fit(X_train, y_train).predict(X_test)\n",
    "    actual = y_test\n",
    "    print('Accuracy: Fold', test_no)\n",
    "    print(lr.score(X_test, y_test))\n",
    "    acc.append(lr.score(X_test, y_test))   \n",
    "    test_no = test_no + 1\n",
    "\n",
    "print('\\nAverage CV accuracy: \\n',np.mean(acc)) \n",
    "    \n",
    "#Store the parameter estimates.\n",
    "cvparams = np.append(lr.coef_, lr.intercept_)\n",
    "print('\\nParameter estimates: \\n')\n",
    "print(cvparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Percentage Accuracy\n",
      "0.9859402460456942 \n",
      "\n",
      "Accuracy: Fold 1\n",
      "0.956140350877193\n",
      "Accuracy: Fold 2\n",
      "0.9385964912280702\n",
      "Accuracy: Fold 3\n",
      "0.9649122807017544\n",
      "Accuracy: Fold 4\n",
      "0.9824561403508771\n",
      "Accuracy: Fold 5\n",
      "0.9823008849557522\n",
      "\n",
      "Average CV accuracy: \n",
      " 0.9648812296227295\n",
      "\n",
      "Parameter estimates: \n",
      "\n",
      "[-2.68117973e+00  2.90861566e-01 -3.38374265e-01  3.45263294e-02\n",
      "  0.00000000e+00 -1.72265722e+01  4.86805790e+00  6.69499875e+01\n",
      " -9.87821851e+00  0.00000000e+00  2.17671461e+00 -1.23688593e+00\n",
      " -4.17604240e-01  1.96898203e-01  0.00000000e+00 -2.76322789e+01\n",
      " -3.92602835e+01  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.83615593e-02  2.45361853e-01  1.07326959e-01  1.64501828e-02\n",
      "  4.83319433e+01 -4.11880539e+00  1.10407917e+01  4.79138185e+01\n",
      "  6.98151052e+00  0.00000000e+00 -7.95429525e+00]\n"
     ]
    }
   ],
   "source": [
    "#LASSO REGRESSION (L1)\n",
    "\n",
    "#LOGISTICREGRESSIONCV is supposed to find the optimal C value\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "#Set the C value lower such that the regularization does have an impact\n",
    "lr = LogisticRegressionCV(penalty='l1', solver = 'liblinear', cv = 5)\n",
    "\n",
    "# Fit the model.\n",
    "fit = lr.fit(predictors, targets)\n",
    "\n",
    "print('Overall Percentage Accuracy')\n",
    "print(lr.score(predictors, targets),'\\n')\n",
    "\n",
    "X = predictors\n",
    "y = targets\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "test_no = 1\n",
    "kf = KFold(n_splits=5) \n",
    "\n",
    "acc = []\n",
    "\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    #Use model defined above\n",
    "    predicted = lr.fit(X_train, y_train).predict(X_test)\n",
    "    actual = y_test\n",
    "    print('Accuracy: Fold', test_no)\n",
    "    print(lr.score(X_test, y_test))  \n",
    "    acc.append(lr.score(X_test, y_test))   \n",
    "    test_no = test_no + 1\n",
    "\n",
    "print('\\nAverage CV accuracy: \\n',np.mean(acc)) \n",
    "    \n",
    "#Store the parameter estimates.\n",
    "cvparams = np.append(lr.coef_, lr.intercept_)\n",
    "print('\\nParameter estimates: \\n')\n",
    "print(cvparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Discussion: \n",
    "\n",
    "I chose to use the LogisiticRegressionCV function because it has a built-in solver that finds the optimal C-value to use for the regularization parameter. \n",
    "\n",
    "Overall, I would opt to use the LASSO regresssion (L1) logisticregressioncv function because it performed more consistently on the cross-validation tests with accuracy ranging from ~94%-98% and averaging 96.5%.\n",
    "\n",
    "In the future, you could also fit the data to a random forest to determine which are the most important features, and then re-run the standard and ridge regression models to see if performance improves on the reduced feature set on test validations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
