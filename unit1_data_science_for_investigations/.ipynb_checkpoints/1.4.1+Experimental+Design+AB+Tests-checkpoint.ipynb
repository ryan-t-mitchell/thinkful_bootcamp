{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental design\n",
    "\n",
    "After the previous emphasis on visualization and putting the \"data\" in Data Science, we're going to change tacks and focus again on a key \"science\" aspect: Experimental Design. Designing an experiment is the only way to get bespoke data, that is, data that is perfectly fitted to the question(s) at hand. Unlike working with existing datasets, properly designed experiments will have all the variables you care about, measured in the way you think is best, collected in a context that emphasizes the features you want to understand.\n",
    "\n",
    "Experimental design can be a powerful tool in your data science arsenal, and like all powerful tools, it needs to be handled the right way. In this lesson, we'll discuss A/B testing and how to evaluate a good experiment, writing a research proposal, and analytical techniques to use when an experiment yields non-normal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Tests\n",
    "\n",
    "There are lots of different possible experimental designs, but A/B testing is probably the most common in business and the one you are most likely to encounter. A/B testing is a technique used to identify whether one version of an object of interest (product, marketing campaign, email text) is better at producing a desired outcome than another.\n",
    "\n",
    "https://cirt.gcu.edu/research/developmentresources/research_ready/experimental/design_types\n",
    "\n",
    "The components of an A/B test are:\n",
    "\n",
    "Two versions of something whose effects will be compared. Typically one version is a \"control version,\" often already in use (or \"no intervention\"), and one is a \"test version,\" which has some changes from the control. That change is often called the treatment. If starting from scratch, however, you may have two different test versions to compare to each other.\n",
    "\n",
    "A sample, divided into two groups. Each sample should be selected so that it is similar to the population you want to understand. The groups should be similar to one another so that any differences between them can be attributed to seeing version A or version B and not something else. You also want the split of between A and B to be as random as possible.\n",
    "\n",
    "A hypothesis. Your hypothesis is what you expect to happen. For example, \"I expect the HTML email will achieve a higher open and conversion rate than the plain text email.\"\n",
    "\n",
    "Outcome(s) of interest. What you expect will change as a result of using version A or version B, and how you will measure that change. This means you have to decide on a _key metric_, which should capture the effect of your change and reflect the motivations for the test in the first place.\n",
    "\n",
    "Other measured variables. This includes information about the two groups that can be used to ensure they are similar, as well as secondary outcomes that are less important than the primary outcomes of interest but which might also change in response to using version A or version B.\n",
    "\n",
    "# Getting a Good Sample\n",
    "\n",
    "One of the key elements of an A/B test, as we said above, is the sample. Particularly important when it comes to actually designing your own experiment is making sure that the two groups, the A and B, are as comparable as possible. The only difference should be the treatment they receive.\n",
    "\n",
    "Now, if you have a constantly occurring easily randomizeable test, where the thing you're interested in measuring is happening all the time in easily separable occurrences, this is pretty easy. Something like marketing emails are usually like this. Thousands of emails are being sent out all the time and you can easily randomize between A and B.\n",
    "\n",
    "However, plenty of things can make this more complex. The most common challenge is a test that either has to be \"all on\" or \"all off\". This could be something like whether playing music in a shop makes people stay longer. You really can't have some people in the shop hearing the music and others not.\n",
    "\n",
    "In that case getting to randomization becomes an art form. You want to think about things like seasonality â€“ making sure there is no difference in your two groups because of day of week or time of year. Perhaps behavior is just different on weekends, at nights, or in the summer.\n",
    "\n",
    "# Keys to Key Metrics\n",
    "\n",
    "The other major sticking point is picking what you want to measure, your outcome. You can, and should, monitor many metrics during most tests, but you typically want to have one key metric that will be your determining factor for whether a test is successful or not.\n",
    "\n",
    "There are several things that make a good key metric. Firstly you want it to be as closely related to your business goal as possible. Measuring clicks is good, but if it makes sense you'd rather measure sales or purchases. This helps prevent getting a result that just affects an intermediate step without improving the actual goal. Just because people engage doesn't mean they'll follow through.\n",
    "\n",
    "You also want your key metric to be reliably measurable. It seems obvious, but many experiments are ruined by measurement error. Ideally this means something that is passively recorded, rather than requiring an additional engagement from subjects or users. You also generally want to avoid self reported data, because that invites its own host of potential biases.\n",
    "\n",
    "As much as possible, you also want to capture the complete effect of your change. Sometimes this may mean time becomes important. Maybe an email campaign doesn't lead to an immediate increase in sign ups, but customers convert later. Trying to include a window of impact that's both reasonable and still consistent across all observations is a valuable characteristic for your metric.\n",
    "\n",
    "# This is a test\n",
    "\n",
    "Let's go through an example. If we were trying to improve the clickthrough rate of an email campaign, we might test two newsletters, one with black italic text and one with green bold text and an exclamation point (\"_Sale Today_\" vs \"Sale Today!\"; our two versions).\n",
    "\n",
    "We would then randomly send one of the two versions of the email to a subset of subscribers to our newsletter (all subscribers who get an email represent our sample, and all subscribers that get the same version of an email represent one of our two experimental groups), and track the number of clicks each version generated (our outcome of interest).\n",
    "\n",
    "We expect that the green bold text will result in more clicks (our hypothesis). We may also want to look at the gender and age of our sample, if we have that information, to make sure the groups are similar. We can also look at whether the two different newsletters affect how quickly people open the email after receiving it (a secondary outcome) as well as the click rate.\n",
    "\n",
    "If the two groups appeared similar in age, gender, and anything else we measured, but click rates were higher for the green bold text version of the newsletter, we would conclude that changing the text to be green and bold caused more clickthroughs. As a result, we would advocate for including green bold text in all future newsletters.\n",
    "\n",
    "# DRILL: Getting Testy...\n",
    "For each of the following questions, outline how you could use an A/B test to find an answer. Be sure to identify all five key components of an A/B test we outlined above.\n",
    "\n",
    "# Does a new supplement help people sleep better?\n",
    "\n",
    "Randomly divide the population into test and control groups. Assess whether the split is truly random by measuring aggregated fields like gender, age, race, occupation, sleep history (if available), etc. \n",
    "\n",
    "Define key metric of success, ie. total hours slept per night. Secondary metrics to measure could include number of times they woke up in the night, time taken to fall asleep, etc.\n",
    "\n",
    "(Try not to rely on self reported data like sleep quality or feeling rested, etc.)\n",
    "\n",
    "Give one group a placebo and another group the new supplement. Measure the key and secondary metrics for a pre-determined period of time. 1-2 weeks seems to be sufficient to draw meaningful conclusions.\n",
    "\n",
    "Compare means and standard deviations of the key metric on the population groups. Take into account confidence levels based on sample sizes, means, and variance (standard deviations) before making final assessment of the supplement's effectiveness.\n",
    "\n",
    "# Will new uniforms help a gym's business?\n",
    "\n",
    "Similar ideas as mentioned above. Additional things to consider are day of week you run the tests on (weekend/weekday seasonality), or time of day if you want to switch uniforms mid-day.\n",
    "\n",
    "Key metric could be new membership signups, membership cancellation rates, attendence, etc. Your outcome measurements would depend on which metric is the key one - etc cancellation rates might take months to measure. Membership signups and attendence could take considerably less time, etc. \n",
    "\n",
    "\n",
    "# Will a new homepage improve my online exotic pet rental business?\n",
    "\n",
    "\n",
    "# If I put 'please read' in the email subject will more people read my emails?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
